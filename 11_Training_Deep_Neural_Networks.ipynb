{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-11 Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradientes Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses\n",
    "down to the lower layers. As a result, the Gradient Descent update leaves the lower\n",
    "layers’ connection weights virtually unchanged, and training never converges to a\n",
    "good solution. We call this the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the oppo‐\n",
    "site can happen: the gradients can grow bigger and bigger until layers get insanely\n",
    "large weight updates and the algorithm diverges. This is the exploding gradients prob‐\n",
    "lem, which surfaces in recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot initialization with a uniform distribution. When creat‐\n",
    "ing a layer, you can change this to He initialization by setting kernel_initial\n",
    "izer=\"he_uniform\" or kernel_initializer=\"he_normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "# [...]\n",
    "# keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "# keras.layers.LeakyReLU(alpha=0.2),\n",
    "# [...]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique consists of\n",
    "adding an operation in the model just before or after the activation function of each\n",
    "hidden layer. This operation simply zero-centers and normalizes each input, then\n",
    "scales and shifts the result using two new parameter vectors per layer: one for scaling,\n",
    "the other for shifting. In other words, the operation lets the model learn the optimal\n",
    "scale and mean of each of the layer’s inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: γ, β, μ, and σ (for\n",
    "example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two\n",
    "parameters, μ and σ, are the moving averages; they are not affected by backpropaga‐\n",
    "tion, so Keras calls them “non-trainable” 9 (if you count the total number of BN\n",
    "parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total\n",
    "number of non-trainable parameters in this model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you create a BN layer in Keras, it also creates two operations that will be\n",
    "called by Keras at each iteration during training. These operations will update the\n",
    "moving averages. Since we are using the TensorFlow backend, these operations are\n",
    "TensorFlow operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'cond/Identity' type=Identity>,\n",
       " <tf.Operation 'cond_1/Identity' type=Identity>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clippingn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is to clip the\n",
    "gradients during backpropagation so that they never exceed some threshold. This is\n",
    "called Gradient Clipping. 12 This technique is most often used in recurrent neural net‐\n",
    "works, as Batch Normalization is tricky to use in RNNs, as we will see in Chapter 15.\n",
    "For other types of networks, BN is usually sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer will clip every component of the gradient vector to a value between\n",
    "–1.0 and 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse Pretrained Layers (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "# model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model_A and model_B_on_A now share some layers. When you train\n",
    "model_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\n",
    "model_A before you reuse its layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, you clone model A’s architecture with\n",
    "clone_model() , then copy its weights (since clone_model() does not clone the\n",
    "weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_A_clone = keras.models.clone_model(model_A)\n",
    "# model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could train model_B_on_A for task B, but since the new output layer was ini‐\n",
    "tialized randomly it will make large errors (at least during the first few epochs), so\n",
    "there will be large error gradients that may wreck the reused weights. To avoid this,\n",
    "one approach is to freeze the reused layers during the first few epochs, giving the new\n",
    "layer some time to learn reasonable weights. To do this, set every layer’s trainable\n",
    "attribute to False and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model_B_on_A.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "# model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
    "# metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must always compile your model after you freeze or unfreeze\n",
    "layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which\n",
    "requires compiling the model again) and continue training to fine-tune the reused\n",
    "layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\n",
    "the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "# validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
    "# model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "\n",
    "# metrics=[\"accuracy\"])\n",
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "#                 validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that transfer learning does not work very well with\n",
    "small dense networks, presumably because small networks learn few patterns, and\n",
    "dense networks learn very specific patterns, which are unlikely to be useful in other\n",
    "tasks. Transfer learning works best with deep convolutional neural networks, which\n",
    "tend to learn feature detectors that are much more general (especially in the lower layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
